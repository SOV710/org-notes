#+title: Speech-to-Text Technology Research Report
#+author: SOV710
#+date: 2025-12-24
#+startup: showall

* 概述

自动语音识别 (Automatic Speech Recognition, ASR), 又称语音转文字 (Speech-to-Text, STT), 是将人类语音信号自动转换为可读文本的技术。近年来在深度学习和大模型推动下, ASR 技术在准确率, 多语言支持, 实时性等方面取得显著突破, 广泛应用于智能客服, 会议记录, 语音助手, 字幕生成等场景。

本调研报告系统梳理当前主流 ASR 技术路线, 分析各类方案的优劣势, 并为不同应用场景提供技术选型建议。

* 技术路线分类

** 传统统计模型路线

*** Hidden Markov Model (HMM)

传统 ASR 系统基于 HMM 建模语音信号与文本的概率关系, 结合高斯混合模型 (GMM) 或深度神经网络 (DNN) 进行声学建模。该路线需要分别训练声学模型, 语言模型和发音词典, 系统架构复杂。

*优势*:
- 理论基础成熟, 可解释性强
- 对小数据集友好
- 可针对特定领域定制优化

*劣势*:
- 需要人工特征工程 (MFCC, PLP 等)
- 系统组件多, 训练和调优复杂
- 泛化能力弱于端到端模型

*** Dynamic Time Warping (DTW)

用于比较和对齐两个时间序列的算法, 常用于模板匹配式语音识别。能够处理变速, 音素时长变化等问题, 但计算复杂度高, 主要用于小词汇量命令识别。

** 端到端深度学习路线

*** CTC (Connectionist Temporal Classification)

通过在输出序列中引入 blank 标签, 允许模型学习输入-输出对齐, 无需预先对齐标注。CTC 是端到端 ASR 的重要基础技术。

*代表模型*:
- DeepSpeech (Mozilla)
- QuartzNet (NVIDIA)
- Conformer

*特点*:
- 无需对齐信息即可训练
- 适合流式识别
- 存在条件独立性假设限制

*** Attention-based Encoder-Decoder

基于注意力机制的序列到序列模型, 编码器提取语音特征, 解码器生成文本, 注意力机制学习对齐关系。

*代表模型*:
- Listen, Attend and Spell (LAS)
- Transformer-based ASR
- Speech-Transformer

*特点*:
- 端到端优化, 性能优异
- 可联合建模语言信息
- 计算复杂度较高
- 原始架构不适合流式场景

*** Hybrid CTC/Attention

结合 CTC 和 Attention 的优势, CTC 提供对齐约束加速收敛, Attention 建模语言依赖提升准确率。这是当前主流端到端 ASR 架构。

*代表框架*:
- ESPnet
- WeNet
- FunASR

*** Transducer (RNN-T, Transformer-Transducer)

基于 Transducer 架构的流式 ASR 模型, 在每个时间步输出零个或多个标记, 天然支持流式识别。

*特点*:
- 原生支持流式推理
- 低延迟, 适合实时应用
- 训练复杂度较高

*** 大语言模型 (LLM) 集成路线

利用预训练大语言模型的语言理解能力提升 ASR 性能, 或将语音编码器与 LLM 联合训练实现语音-文本多模态理解。

*代表模型*:
- Whisper (OpenAI) - 68万小时多语言数据预训练
- USM (Google) - 通用语音模型
- FireRedASR-LLM (小红书) - LLM 增强中文 ASR

*特点*:
- 零样本/少样本学习能力强
- 多语言, 多任务 (转录 + 翻译) 支持
- 对噪声, 口音鲁棒性好
- 模型规模大, 资源需求高

* 主流开源方案对比

** OpenAI Whisper

*** 技术特点

- *架构*: Encoder-Decoder Transformer
- *训练数据*: 68万小时多语言, 多任务监督数据
- *模型规模*: 5个版本 (tiny 39M → large 1.5B 参数)
- *语言支持*: 99种语言转录 + 多语言到英文翻译
- *准确率*: LibriSpeech clean 测试集 WER 2.7% (large 模型)

*** 优势

- 开箱即用, 零样本泛化能力极强
- 多语言支持最全面
- 噪声鲁棒性优异
- 单模型多任务 (转录 + 翻译)
- 时间戳支持

*** 劣势

- 原版单音频 <30 秒长度限制
- 缺少说话人分类, 词级时间戳
- 存在幻觉现象 (重复, 捏造内容)
- 实时率 (RTF) 较高, 不适合低延迟场景
- 针对特定领域需微调

*** 适用场景

- 多语言内容转录
- 离线批处理任务
- 研究原型开发
- 对准确率要求高, 延迟容忍度大的场景

** Meta Wav2Vec 2.0

*** 技术特点

- *架构*: 自监督学习 + Transformer
- *训练方式*: 无标注数据预训练 + 少量标注数据微调
- *语言覆盖*: 低资源语言友好

*** 优势

- 利用海量无标注数据
- 低资源语言效果好
- 预训练模型可迁移

*** 劣势

- 需要微调才能实用
- 实时推理性能一般
- 部署复杂度较高

*** 适用场景

- 低资源语言 ASR
- 需要定制语言模型的场景
- 研究和实验

** Mozilla DeepSpeech

*** 技术特点

- *架构*: RNN + CTC
- *轻量化*: 模型小, 适合边缘设备

*** 优势

- 模型轻量 (47MB 左右)
- 离线运行, 隐私保护
- 适合资源受限环境

*** 劣势

- *严重限制*: 单音频限制 10-20 秒
- 文本语料库小 (每句 ~14 词/~100 字符)
- 准确率低于 Whisper 等新模型
- 项目已停止维护 (2021)

*** 适用场景

- 命令词识别
- 短音频转录
- 嵌入式设备 (历史选择, 新项目不推荐)

** Kaldi

*** 技术特点

- *定位*: ASR 开发工具包, 非即用模型
- *架构*: 模块化, 支持传统和深度学习方法
- *数据支持*: LDC 等大型语料库

*** 优势

- 高度可定制
- 科研和工业级性能
- 活跃社区, 丰富 recipe
- 跨平台 (PC, Android, WebAssembly)

*** 劣势

- 学习曲线陡峭 (需 shell, 信号处理知识)
- 从零开始部署复杂
- 不适合快速原型开发

*** 适用场景

- 科研项目
- 需要深度定制的企业级系统
- 构建特定领域 ASR

** SpeechBrain

*** 技术特点

- *定位*: PyTorch 全栈语音工具包
- *功能*: ASR + TTS + Speaker Recognition + 语言模型
- *架构*: 统一 PyTorch 接口

*** 优势

- 一站式语音解决方案
- 支持 Whisper, Wav2Vec 微调
- 社区贡献模型丰富
- 开发体验好

*** 劣势

- 社区模型质量参差不齐
- 企业级部署需严格测试

*** 适用场景

- 对话式 AI 系统
- 多任务语音处理
- 快速原型和研究

** ESPnet

*** 技术特点

- *架构*: 端到端 PyTorch/Chainer 工具包
- *多任务*: ASR + TTS + ST + 多模态
- *训练方式*: Hybrid CTC/Attention

*** 优势

- 统一接口支持流式和非流式
- 多任务联合训练
- 科研前沿技术集成

*** 劣势

- 部署复杂度较高
- 需要深度学习背景

*** 适用场景

- 复杂多任务语音系统
- Speech-to-speech translation
- 学术研究

** 国产开源方案

*** FunASR (阿里达摩院)

- Paraformer: 高效非自回归 ASR
- SenseVoice: 多语言 + 情感识别 + 流式推理
- 支持 VAD, 标点恢复, 说话人分离
- 70ms 内处理 10 秒音频
- 热词定制, 模型微调支持

*** PaddleSpeech (百度飞桨)

- 全流程 ASR/TTS 工具包
- 流式和非流式统一接口
- WebSocket 协议支持
- 国内场景优化

* 商业 API 服务对比

** 云厂商方案

*** 百度智能云

- *模型*: Deep Peak2 端到端
- *准确率*: 近场中文普通话 98%
- *接口*: WebSocket API, SDK (Android/iOS/Linux)
- *特色*: 智能断句, 智能标点, VAD 时间戳

*** 阿里云

- *模型*: LC-BLSTM/DFSMN-CTC (相比传统 CTC 降低 20% 错误率)
- *特色*: LFR 解码技术 (解码速率提升 3 倍)
- *部署*: 支持本地化部署

*** 腾讯云

- *接口*: WebSocket 实时语音识别
- *特色*: 脏词过滤, 智能断句
- *安全*: HMAC-SHA1 签名认证

*** 华为云

- *模式*: 流式一句话/连续/单句模式
- *特色*: 定制垂直领域语言模型, 时间戳, 智能标点

*** 科大讯飞

- 成熟商业方案
- 垂直行业深耕 (医疗, 教育等)
- SDK 和云服务双支持

** 国际厂商

*** Microsoft Azure Speech Service

- 多语言支持
- Speaker Recognition 集成
- REST API + SDK

*** Google Cloud Speech-to-Text

- Chirp 通用模型
- 自适应语言模型
- 流式和批处理

*** AWS Transcribe

- 医疗, 呼叫中心专业模型
- 实时和批处理
- 说话人分离

*** Deepgram

- Nova-2 模型
- 高吞吐量
- 端到端优化

* 部署架构方案

** 云端部署

*** 架构特点

- 集中式服务器处理
- 音频流上传 → 云端识别 → 结果返回
- WebSocket/gRPC 长连接

*** 优势

- *算力强大*: 支持大模型 (Whisper Large, 数十亿参数)
- *可扩展性*: 弹性伸缩, 应对高并发
- *维护简单*: 统一更新模型
- *功能完整*: 说话人分离, 多语言, 翻译等高级功能

*** 劣势

- *延迟高*: 网络往返 100-500ms
- *成本高*: 按调用次数/时长计费, 长期成本不可控
- *隐私风险*: 敏感数据上传 (医疗, 金融场景不可接受)
- *网络依赖*: 弱网/离线环境失效

*** 适用场景

- 非实时批处理 (会议录音转写, 字幕生成)
- 低频调用场景
- 多语言, 多功能需求
- 无隐私敏感数据

** 边缘设备部署

*** 架构特点

- 模型部署在本地服务器/边缘计算节点
- 数据不出本地网络
- GPU/NPU 加速推理

*** 硬件需求

- *入门级*: NVIDIA Jetson Orin (32GB, 256TOPS) - 支持 Whisper Small/Base
- *高性能*: NVIDIA A100/H100 多卡 - 支持 Large 模型高并发
- *国产芯片*: 海光, 寒武纪, 爱芯元智 AX650N

*** 优势

- *低延迟*: <200ms 响应
- *隐私保护*: 数据零外传
- *成本可控*: 一次性硬件投入, 无持续费用
- *离线运行*: 无网络依赖

*** 劣势

- *硬件成本*: 初期投入高 (5-50 万元级别)
- *模型受限*: 需模型压缩 (剪枝, 量化, 蒸馏)
- *维护负担*: 需专业团队运维
- *算力天花板*: 无法像云端弹性扩展

*** 技术优化

- *模型量化*: FP32 → INT8 (TensorRT), 速度提升 3 倍
- *模型剪枝*: 移除冗余参数, 减小模型体积
- *推理加速*: ONNX Runtime, TensorRT, OpenVINO
- *批处理*: 提高 GPU 利用率

*** 适用场景

- 隐私敏感场景 (医疗, 金融, 政务)
- 实时性要求高 (智能客服, 实时字幕)
- 长期高频使用 (呼叫中心)
- 定制化需求 (行业术语, 方言)

** 嵌入式系统部署

*** 架构特点

- 单芯片集成 ASR (MCU, SoC)
- 超低功耗 (<1W)
- 极小模型 (<30MB)

*** 硬件平台

- *MCU*: ESP32, STM32
- *SoC*: 全志 R329, 瑞芯微 RK3568
- *专用芯片*: 语音 DSP, NPU

*** 优势

- *超低功耗*: 适合电池供电设备
- *低成本*: 芯片成本 <10 元
- *快速响应*: <100ms
- *隐私安全*: 本地处理
- *一体化*: ASR + TTS 集成封装

*** 劣势

- *识别能力弱*: 仅支持命令词/短句 (10-20 秒)
- *准确率低*: 70-85% (vs 云端 95%+)
- *无复杂功能*: 不支持长文本, 流式对话, 多语言
- *定制化难*: 模型更新复杂

*** 推荐方案

- Picovoice Rhino (命令词识别)
- Google WakeWord Engine
- EdgeImpulse + Coqui TTS (模型剪裁)
- Moonshine (Useful Sensors, 27M/61M 参数, 仅英文)

*** 适用场景

- 智能家居 (灯控, 家电)
- 可穿戴设备
- 玩具, 机器人
- 工业控制命令

* 流式 vs 批处理

** 流式识别 (Streaming ASR)

*** 定义

音频流实时传输, 边说边识别边返回结果, 无需等待完整语句结束。

*** 技术要点

- *Chunk 切分*: 固定上下文窗口 (1-2 秒), 避免 RTF > 1.0
- *VAD (Voice Activity Detection)*: 自动断句
- *增量解码*: 输出中间结果 + 最终结果

*** 实现方式

*方式 1: 调整模型结构*
- Streaming Transformer (限制 attention 范围)
- RNN-Transducer (天然流式)
- Conformer with limited right context

*方式 2: 修改训练方式*
- Chunk-wise training
- Lookahead mechanism
- Truncated BPTT

*** 协议

- *WebSocket*: 双工通信, 适合长连接
- *gRPC Streaming*: 高性能 RPC

*** 性能指标

- *实时率 (RTF)*: 必须 ≤ 1.0
- *首字延迟*: 从开始说话到第一个字输出 (<500ms)
- *尾字延迟*: 说完到最终结果 (<1s)

*** 典型应用

- 实时字幕 (直播, 会议)
- 语音助手
- 呼叫中心实时质检
- 同声传译

** 批处理识别 (Offline/Batch ASR)

*** 定义

处理完整音频文件, 一次性返回结果, 可利用完整上下文信息。

### 优势

- *准确率高*: 利用全局上下文
- *后处理完善*: 标点, 大小写, 数字归一化
- *批量优化*: 并行处理提高吞吐

### 适用场景

- 会议录音转写
- 视频字幕生成
- 历史音频归档检索

** 混合架构

实际项目推荐流式 ASR (实时反馈) + 批处理优化 (离线精修) 的混合方案:

1. 实时场景用流式 ASR 即时展示
2. 会话结束后批处理全文优化
3. 结合 NLP 后处理提升可读性

* 技术选型决策框架

** 准确率需求

*** 高准确率 (>95%)

- *首选*: Whisper Large, 商业 API (百度, 阿里)
- *场景*: 医疗记录, 法庭记录, 专业字幕
- *代价*: 延迟高, 成本高

*** 中等准确率 (85-95%)

- *首选*: Whisper Small/Medium, FunASR Paraformer
- *场景*: 会议记录, 语音笔记
- *平衡*: 性能与成本

*** 低准确率可接受 (<85%)

- *首选*: 嵌入式方案, 命令词识别
- *场景*: 智能家居, 简单交互

** 延迟要求

*** 超低延迟 (<200ms)

- *方案*: 边缘部署流式 ASR (RNN-T, Streaming Conformer)
- *硬件*: GPU 加速
- *场景*: 语音助手, 实时翻译

*** 低延迟 (200-500ms)

- *方案*: 云端流式 API, 优化网络
- *场景*: 实时字幕, 客服系统

*** 延迟容忍 (>1s)

- *方案*: 批处理, Whisper
- *场景*: 离线转写

** 语言支持

*** 多语言 (50+)

- *首选*: Whisper, 商业多语言 API
- *特色*: Whisper 支持 99 种语言

*** 中英文

- *首选*: FunASR, PaddleSpeech, 国内云厂商
- *优势*: 中文优化

*** 单一语言

- *首选*: 针对该语言微调的专用模型
- *效果*: 最优

*** 方言/低资源语言

- *首选*: Wav2Vec 2.0 预训练 + 少量数据微调
- *挑战*: 需要数据积累

** 成本预算

*** 低成本/零成本

- *方案*: 开源模型 (Whisper, Vosk) + 自建服务
- *硬件*: 消费级 GPU (RTX 3090, 4090)
- *人力*: 需技术团队

*** 中等成本

- *方案*: 云 API 按量付费
- *适用*: 中低频调用

*** 高成本可接受

- *方案*: 企业定制, 专有部署
- *服务*: 全托管 SaaS

** 隐私安全

*** 极高敏私要求

- *方案*: 本地部署, 数据不出内网
- *场景*: 医疗, 军工, 金融核心业务
- *方案*: 边缘设备, 离线 Whisper

*** 一般隐私要求

- *方案*: 云 API (正规厂商), 数据加密传输
- *合规*: GDPR, 等保认证

** 定制化需求

*** 行业术语 (医疗, 法律, 工业)

*技术路线*:
- 热词定制 (FunASR, 云 API)
- 领域语言模型微调
- 自有数据训练/微调 (Whisper, Kaldi)

*数据需求*:
- 最少 1000+ 条真实语料
- 高质量转写标注

*** 特定说话人

- 说话人自适应
- 声纹识别预处理

*** 方言

- Wav2Vec 2.0 + 方言语料微调
- 区域云厂商方言支持

** 硬件资源

*** 高性能服务器 (GPU 集群)

- *可选*: Whisper Large, 多语言大模型
- *并发*: 高吞吐批处理

*** 边缘设备 (单卡 GPU, Jetson)

- *可选*: Whisper Tiny/Small, FunASR, 量化模型
- *优化*: INT8 量化, TensorRT

*** 普通 PC (CPU only)

- *可选*: Vosk, DeepSpeech, whisper.cpp (C++ 优化版)
- *性能*: RTF 0.5-2.0

*** 嵌入式 (MCU, 低功耗 SoC)

- *可选*: Picovoice, Moonshine, 命令词引擎
- *限制*: 词汇量小, 短句

** 开发效率

*** 快速原型

- *首选*: Whisper API, 云 API
- *时间*: 小时级

*** 可控迭代

- *首选*: SpeechBrain, ESPnet, FunASR
- *时间*: 周级

*** 深度定制

- *首选*: Kaldi, 自研
- *时间*: 月级

* 实施建议

** 需求分析清单

1. *应用场景*: 实时 vs 离线? 交互 vs 转写?
2. *语言*: 单语 vs 多语? 方言?
3. *音频环境*: 近场 vs 远场? 噪声水平?
4. *准确率目标*: 关键指标 (WER/CER) 要求?
5. *延迟容忍度*: 实时性要求?
6. *并发量*: 峰值 QPS?
7. *数据隐私*: 本地 vs 云端?
8. *预算*: 硬件 + 软件 + 人力?
9. *定制需求*: 行业术语? 特殊功能?
10. *技术栈*: 团队能力? 已有基础设施?

** 技术验证流程

1. *POC 测试*: 用真实数据测试 2-3 个候选方案
2. *准确率评估*: 标注测试集, 计算 WER/CER
3. *性能测试*: RTF, 延迟, 吞吐量
4. *成本估算*: TCO (Total Cost of Ownership) 分析
5. *风险评估*: 技术风险, 合规风险

** 混合架构设计

*推荐策略*:

- *核心场景*: 边缘部署 ASR (低延迟, 隐私)
- *复杂任务*: 云端大模型 (多语言, 翻译)
- *离线优化*: 批处理精修 (高准确率)

*示例*:

智能客服系统:
1. 边缘流式 ASR (FunASR) → 实时话术识别
2. 云端 Whisper Large → 离线质检分析
3. 定制热词库 → 业务术语优化

** 后处理增强

原始 ASR 输出通常无标点, 大小写, 需后处理提升可读性:

1. *标点恢复*: BERT 标点模型
2. *大小写规范*: 规则 + 模型
3. *文本归一化 (ITN)*: 数字, 日期, 单位等
4. *说话人分离*: Diarization (pyannote.audio)
5. *敏感信息脱敏*: PII 检测和替换

** 持续优化

1. *数据闭环*: 收集badcase, 标注, 微调
2. *A/B 测试*: 对比不同模型/参数配置
3. *监控告警*: 准确率, 延迟, 可用性
4. *模型更新*: 定期升级基础模型

* 典型应用场景方案

** 场景 1: 视频会议实时字幕

*需求*:
- 低延迟 (<500ms)
- 中英文支持
- 噪声鲁棒
- 多人对话

*推荐方案*:

- *ASR*: FunASR Paraformer (流式) / Azure Speech Service
- *部署*: 边缘服务器 + GPU
- *协议*: WebRTC 音频采集 → WebSocket ASR
- *增强*: VAD 断句, 说话人分离

*技术栈*:

#+begin_src text
WebRTC (音频采集)
  ↓
WebSocket Client
  ↓
边缘 ASR 服务 (FunASR + GPU)
  ↓
实时字幕推流
#+end_src

** 场景 2: 医疗病历录入

*需求*:
- 极高准确率 (>98%)
- 医疗术语支持
- 隐私合规 (数据不出院)
- 方言适配

*推荐方案*:

- *ASR*: Whisper Medium + 医疗语料微调
- *部署*: 院内服务器 (Jetson Orin / A100)
- *定制*: 医疗热词库, 术语语言模型
- *后处理*: 医学 NER, 实体链接

*数据准备*:

- 收集 200+ 小时医疗对话录音
- 标注医疗术语, 药品名, 诊断等
- Whisper 微调 (LoRA) 适配

** 场景 3: 智能音箱命令识别

*需求*:
- 超低延迟 (<200ms)
- 超低功耗
- 离线运行
- 命令词 (~100 个)

*推荐方案*:

- *ASR*: Picovoice Rhino (命令词) / 定制小模型
- *硬件*: ESP32-S3 / 全志 R329
- *架构*: 唤醒词 (KWS) + 命令识别 (ASR)
- *功耗*: <1W

*流程*:

#+begin_src text
麦克风阵列
  ↓
唤醒词检测 (KWS, 本地)
  ↓
命令词 ASR (本地 / 云端混合)
  ↓
意图识别 (NLU)
  ↓
设备控制
#+end_src

** 场景 4: 多语言视频字幕生成

*需求*:
- 支持 50+ 语言
- 高准确率
- 时间戳对齐
- 翻译 + 配音

*推荐方案*:

- *ASR*: Whisper Large (多语言 + 翻译)
- *部署*: 云端批处理 (GPU 集群)
- *工作流*: ASR → 翻译 (NMT) → TTS → 字幕合成
- *工具*: FFmpeg (字幕封装)

*优化*:

- VAD 预处理减少静音段
- 句级切分提升并行度
- 缓存常见语言模型

** 场景 5: 呼叫中心质检

*需求*:
- 双轨录音 (坐席 + 客户)
- 说话人分离
- 情绪识别
- 关键词检测

*推荐方案*:

- *ASR*: FunASR (VAD + ASR + Speaker Diarization)
- *部署*: 本地机房 (合规要求)
- *后处理*: 情感分析, 敏感词检测
- *存储*: 音频 + 文本归档

*技术栈*:

- 实时流式 ASR (坐席监控)
- 离线批处理 ASR (全量质检)
- BERT 情感分类
- ElasticSearch 文本检索

* 前沿技术趋势

** 端到端神经编解码器

将音频直接编码为语义 token, 跳过文本中间表示, 实现语音到语音的端到端建模 (speech-to-speech without text)。

*代表*: AudioLM, VALL-E, SpeechGPT

** 多模态融合

语音 + 视觉 (唇语) + 文本多模态输入, 提升噪声鲁棒性和理解能力。

*应用*: 视频会议, AR/VR 交互

** 超大规模预训练

数十万小时, 数百种语言的超大规模预训练, 实现真正的通用语音基座模型。

*代表*: Google USM (Universal Speech Model), Meta MMS

** 边缘 AI 专用芯片

集成 NPU 的语音专用 SoC, 支持 Transformer 推理, 功耗 <500mW。

*代表*: 高通 Hexagon DSP, 联发科 APU

** Zero-shot 说话人适配

无需训练即可适配新说话人风格, 提升个性化体验。

** 联邦学习隐私保护

分布式训练保护用户隐私, 同时持续优化模型。

* 附录

** 常用评估指标

*** Word Error Rate (WER)

=WER = (S + D + I) / N=

- S: Substitution (替换错误)
- D: Deletion (删除错误)
- I: Insertion (插入错误)
- N: 参考文本词数

英文常用, 中文用 CER (Character Error Rate)。

*** Real-Time Factor (RTF)

=RTF = 处理时间 / 音频时长=

RTF < 1.0 表示可实时处理。

*** 延迟指标

- *首字延迟*: 开始说话 → 第一个字输出
- *尾字延迟*: 说完 → 最终结果

** 开源数据集

- *LibriSpeech*: 1000 小时英文有声书
- *Common Voice*: Mozilla 众包多语言
- *AISHELL*: 170 小时中文普通话
- *WenetSpeech*: 10000+ 小时中文多领域
- *GigaSpeech*: 10000 小时英文 podcast
- *VoxPopuli*: 欧洲议会多语言

** 相关工具链

- *音频处理*: librosa, soundfile, pydub
- *VAD*: WebRTC VAD, pyannote.audio
- *推理优化*: ONNX Runtime, TensorRT, OpenVINO
- *服务框架*: FastAPI, gRPC, WebSocket
- *容器化*: Docker, Kubernetes
- *监控*: Prometheus + Grafana

** 参考资源

*** 官方文档

- OpenAI Whisper: [[https://github.com/openai/whisper]]
- FunASR: [[https://github.com/alibaba-damo-academy/FunASR]]
- ESPnet: [[https://github.com/espnet/espnet]]
- Kaldi: [[https://kaldi-asr.org/]]

*** 学术资源

- arXiv Speech Processing
- Interspeech 会议论文
- ICASSP 会议论文

*** 社区

- /r/MachineLearning
- Hugging Face Forums
- GitHub Discussions

* 总结

语音转文字技术已进入成熟应用阶段, 开源方案 (Whisper, FunASR 等) 和商业 API 并行发展, 为不同场景提供了丰富选择。技术选型需综合考虑准确率, 延迟, 成本, 隐私, 定制化等多维度因素, 没有一劳永逸的方案, 需根据实际需求权衡取舍。

*关键建议*:

1. *先验证后决策*: 用真实数据 POC 测试
2. *混合架构*: 云端 + 边缘 + 嵌入式各司其职
3. *数据驱动*: 持续收集 badcase, 迭代优化
4. *开源优先*: 降低成本, 保持技术自主可控
5. *关注趋势*: 大模型, 多模态是未来方向

随着大语言模型和多模态技术发展, ASR 将从单纯的"语音转文字"工具, 演进为"语音理解"和"语音交互"的智能基座, 值得持续关注和投入。
